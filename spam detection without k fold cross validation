import os
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    accuracy_score, precision_score,
    recall_score, f1_score, roc_auc_score
)
import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import BertTokenizer, BertModel

# ------------------------------
# 0. Select Device (GPU #1)
# ------------------------------
device = torch.device("cuda:1" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# ------------------------------
# 1. Load & Clean Data
# ------------------------------
df = pd.read_excel('test.xlsx', sheet_name='Sheet1')
# expected columns: reviewText, overall, userID, productID, class
df['clean_text'] = (
    df['reviewText']
      .astype(str)
      .str.lower()
      .str.replace(r'[^a-z0-9\s]', ' ', regex=True)
      .str.replace(r'\s+', ' ', regex=True)
      .str.strip()
)

# ------------------------------
# 2. BERT Sentence Embeddings
# ------------------------------
tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')
bert_model = BertModel.from_pretrained('bert-base-multilingual-cased').to(device)
bert_model.eval()

def get_bert_embeddings(texts, batch_size=16, max_length=200):
    embeddings = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        enc = tokenizer(batch,
                        padding=True,
                        truncation=True,
                        max_length=max_length,
                        return_tensors='pt').to(device)
        with torch.no_grad():
            out = bert_model(**enc)
            cls_emb = out.last_hidden_state[:,0,:]   # [CLS] token
        embeddings.append(cls_emb.cpu())
    return torch.cat(embeddings, dim=0)

print("Computing BERT embeddings...")
review_embeddings = get_bert_embeddings(df['clean_text'].tolist())
X_bert = review_embeddings.numpy()  # for clustering

# ------------------------------
# 3. Build Review Graph via KMeans
# ------------------------------
n_clusters = 10
km = KMeans(n_clusters=n_clusters, random_state=42)
clusters = km.fit_predict(X_bert)

# adjacency: same cluster -> connect
A = (clusters[:,None] == clusters[None,:]).astype(float)

def normalize_adj(A):
    A_hat = A + np.eye(A.shape[0])
    D = np.sum(A_hat, axis=1)
    D_inv_sqrt = np.power(D, -0.5)
    D_inv_sqrt[np.isinf(D_inv_sqrt)] = 0.
    D_inv_sqrt = np.diag(D_inv_sqrt)
    return D_inv_sqrt @ A_hat @ D_inv_sqrt

A_norm = torch.tensor(normalize_adj(A), dtype=torch.float32, device=device)

# ------------------------------
# 4. User & Item Features
# ------------------------------
u_stats = df.groupby('userID')['overall'].agg(
    u_count='count', u_mean='mean', u_std='std'
).fillna(0)
df = df.join(u_stats, on='userID')

i_stats = df.groupby('productID')['overall'].agg(
    i_count='count', i_mean='mean', i_std='std'
).fillna(0)
df = df.join(i_stats, on='productID')

user_cols = ['u_count','u_mean','u_std']
item_cols = ['i_count','i_mean','i_std']
scaler_u = StandardScaler()
scaler_i = StandardScaler()
df[user_cols] = scaler_u.fit_transform(df[user_cols])
df[item_cols] = scaler_i.fit_transform(df[item_cols])

# ------------------------------
# 5. Tensors for Model
# ------------------------------
review_feats = review_embeddings.to(device)                       # (N,768)
user_feats   = torch.tensor(df[user_cols].values, device=device)  # (N,3)
item_feats   = torch.tensor(df[item_cols].values, device=device)  # (N,3)
labels       = torch.tensor(df['class'].values,   device=device)  # (N,)

# ------------------------------
# 6. Model Definition
# ------------------------------
class GCNLayer(nn.Module):
    def __init__(self, in_dim, out_dim):
        super().__init__()
        self.lin = nn.Linear(in_dim, out_dim)
    def forward(self, X, A):
        h = self.lin(X)
        h = A @ h
        return F.relu(h)

class GDFN_BERT(nn.Module):
    def __init__(self, rev_dim, gcn_dim, u_dim, i_dim, fusion_dim, n_classes):
        super().__init__()
        self.gcn1 = GCNLayer(rev_dim, gcn_dim)
        self.gcn2 = GCNLayer(gcn_dim, gcn_dim)
        self.u_mlp = nn.Sequential(nn.Linear(u_dim, gcn_dim), nn.ReLU())
        self.i_mlp = nn.Sequential(nn.Linear(i_dim, gcn_dim), nn.ReLU())
        self.fuse  = nn.Sequential(nn.Linear(gcn_dim**3, fusion_dim), nn.ReLU())
        self.cls   = nn.Linear(fusion_dim, n_classes)
    def forward(self, R, A, U, I):
        h = self.gcn1(R, A)
        h = self.gcn2(h, A)                # (N, gcn_dim)
        u = self.u_mlp(U)                  # (N, gcn_dim)
        i = self.i_mlp(I)                  # (N, gcn_dim)
        B, d = h.shape
        h3 = h.view(B, d, 1, 1)
        u3 = u.view(B, 1, d, 1)
        i3 = i.view(B, 1, 1, d)
        fused = (h3 * u3 * i3).view(B, -1) # (N, d^3)
        fused = self.fuse(fused)           # (N, fusion_dim)
        return self.cls(fused)            # (N,2)

# ------------------------------
# 7. Training & Evaluation
# ------------------------------
model = GDFN_BERT(
    rev_dim=review_feats.size(1),
    gcn_dim=64,
    u_dim=user_feats.size(1),
    i_dim=item_feats.size(1),
    fusion_dim=128,
    n_classes=2
).to(device)

opt = torch.optim.Adam(model.parameters(), lr=2e-4)
crit = nn.CrossEntropyLoss()

def train(epochs=50):
    for ep in range(1, epochs+1):
        model.train()
        opt.zero_grad()
        out = model(review_feats, A_norm, user_feats, item_feats)
        loss = crit(out, labels)
        loss.backward()
        opt.step()
        if ep % 10 == 0:
            preds = out.argmax(dim=1)
            acc = (preds == labels).float().mean()
            print(f"Epoch {ep:02d}/{epochs}  Loss={loss:.4f}  Acc={acc:.4f}")

def evaluate():
    model.eval()
    with torch.no_grad():
        out = model(review_feats, A_norm, user_feats, item_feats)
        probs = F.softmax(out, dim=1)[:,1].cpu().numpy()
        preds = out.argmax(dim=1).cpu().numpy()
    y = labels.cpu().numpy()
    print("Accuracy :", accuracy_score(y, preds))
    print("Precision:", precision_score(y, preds))
    print("Recall   :", recall_score(y, preds))
    print("F1 Score :", f1_score(y, preds))
    print("AUC      :", roc_auc_score(y, probs))

if __name__ == "__main__":
    print("Starting training on device", device)
    train(epochs=50)
    print("Evaluation on full dataset:")
    evaluate()
